{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46030170-c5ec-47ed-a6e6-87a6de7fe55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overfitting occurs when a machine learning model is too complex and fits the training data too closely, resulting in poor performance on new data. \n",
    "# Underfitting occurs when the model is too simple and unable to capture the underlying patterns in the data, leading to poor performance on both training and new data. \n",
    "# Overfitting can be mitigated by reducing the complexity of the model, using regularization techniques, or increasing the amount of training data. \n",
    "# Underfitting can be addressed by increasing the complexity of the model, using a different algorithm, or adding more features to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87db1f82-3af0-467f-b264-f18e85839734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To reduce overfitting, one can reduce the complexity of the model, such as using fewer features, adding regularization techniques like L1 or L2 regularization, \n",
    "# or using dropout layers in neural networks. Other approaches include increasing the amount of training data or using data augmentation techniques to generate more training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22c41139-1031-4975-af0d-823e3c03849f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Underfitting occurs when the model is too simple and unable to capture the underlying patterns in the data. This can happen when there are too few features in the data, or \n",
    "# when the model is too constrained by the regularization techniques. Underfitting can also occur when the algorithm is not appropriate for the data, such as using linear \n",
    "# regression for non-linear problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4a29989-f220-4545-ab35-25394c8e74dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The bias-variance tradeoff refers to the tradeoff between the model's ability to fit the training data well (low bias) and its ability to generalize to new data (low variance). \n",
    "# High bias models are too simple and unable to capture the underlying patterns in the data, while high variance models are too complex and fit the training data too closely, \n",
    "# leading to poor performance on new data. The optimal model lies in the middle, with a balance between bias and variance that allows for good performance on both training and new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd425224-378e-42ea-b787-a74de4036f38",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Common methods for detecting overfitting and underfitting include examining the model's training and validation performance, using cross-validation to estimate the model's \n",
    " #    performance on new data, and plotting learning curves to see how the model's performance changes as the amount of training data increases. \n",
    " #    Overfitting can be detected when the model performs well on the training data but poorly on the validation data,\n",
    " #    while underfitting can be detected when the model performs poorly on both training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f417ffb-835b-4c72-98a9-26009fdb82bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bias refers to the model's tendency to make systematic errors, while variance refers to the model's tendency to make random errors. High bias models are too simple \n",
    "# and unable to capture the underlying patterns in the data, while high variance models are too complex and fit the training data too closely. \n",
    "# Examples of high bias models include linear regression for non-linear problems, while examples of high variance models include complex neural networks or decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a66c916c-680f-4dfa-8206-fefba76fa511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularization is a technique used in machine learning to prevent overfitting by adding a penalty term to the model's loss function. \n",
    "# Common regularization techniques include L1 and L2 regularization, which add a penalty term based on the magnitude of the model's parameters, \n",
    "# and dropout, which randomly drops out some of the model's neurons during training to reduce co-adaptation. Other techniques include early stopping, \n",
    "# which stops the training process when the model starts to overfit, and data augmentation, which generates new training examples by applying random \n",
    "# transformations to the existing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ee136d-4893-44b5-9777-8d605154a7c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
